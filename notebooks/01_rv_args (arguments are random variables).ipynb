{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rv_args: Arguments are random variables\n",
    "\n",
    "> Using Python dataclass and optuna distribution to define arguments of a function, in order to enable documentatable, easy and pythonic way to handle hyperparameters optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rv_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用dataclass，要求传入函数的参数是强类型，而且有一个随机概率分布，这样方便后面定义调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from dataclasses import dataclass, field, MISSING, _MISSING_TYPE, fields, asdict\n",
    "from typing import List, Dict, Any, Type, Optional, Callable, Union\n",
    "from optuna.distributions import BaseDistribution, distribution_to_json, json_to_distribution\n",
    "\n",
    "rv_dataclass_metadata_key = \"thu_rv\"\n",
    "rv_missing_value = \"thu_rv_missing\"\n",
    "\n",
    "@dataclass\n",
    "class PythonField:\n",
    "    default:Any = rv_missing_value# The default value of the field\n",
    "    default_factory:Callable[[], Any] = rv_missing_value# A function to generate the default value of the field\n",
    "    init:bool=True\n",
    "    repr:bool=True\n",
    "    hash:Union[None, bool]=None\n",
    "    compare:bool=True\n",
    "    metadata:Union[Dict[str, Any], None]=None\n",
    "    # kw_only:Union[_MISSING_TYPE, bool]=MISSING\n",
    "    kw_only:Union[None, bool]=rv_missing_value\n",
    "    def __post_init__(self):\n",
    "        # print(self)\n",
    "        if self.default == rv_missing_value:\n",
    "            self.default = MISSING\n",
    "        if self.default_factory == rv_missing_value:\n",
    "            self.default_factory = MISSING\n",
    "        if self.kw_only == rv_missing_value:\n",
    "            self.kw_only = MISSING\n",
    "        # self.default = self.default or MISSING\n",
    "        # self.default_factory = self.default_factory or MISSING\n",
    "        # self.kw_only = self.kw_only or MISSING\n",
    "    def __call__(self, **kwargs: Any) -> Any:\n",
    "        if self.metadata is None:\n",
    "            # self.metadata = {**kwargs}\n",
    "            metadata = {**kwargs}\n",
    "        \n",
    "        return field(default=self.default, \n",
    "                     default_factory=self.default_factory, \n",
    "                     init=self.init, \n",
    "                     repr=self.repr, \n",
    "                     hash=self.hash, \n",
    "                     compare=self.compare, \n",
    "                     metadata=metadata, \n",
    "                     kw_only=self.kw_only)\n",
    "    def __invert__(self):\n",
    "        return self()\n",
    "\n",
    "@dataclass\n",
    "class RandomVariable(PythonField):\n",
    "    description: str = \"MISSING description. \"# The description of the field\n",
    "    distribution:BaseDistribution = \"MISSING distribution. \"# The distribution of the data\n",
    "    def __call__(self, **kwargs: Any) -> Any:\n",
    "        return super().__call__(description=self.description, distribution=self.distribution, \n",
    "                                **{rv_dataclass_metadata_key: self}, \n",
    "                                **kwargs)\n",
    "    def __invert__(self):\n",
    "        return self()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': <dataclasses._MISSING_TYPE at 0x7fae5a566590>,\n",
       " 'default_factory': <dataclasses._MISSING_TYPE at 0x7fae5a564df0>,\n",
       " 'init': True,\n",
       " 'repr': True,\n",
       " 'hash': None,\n",
       " 'compare': True,\n",
       " 'metadata': None,\n",
       " 'kw_only': <dataclasses._MISSING_TYPE at 0x7fae5a566290>,\n",
       " 'description': 'MISSING description. ',\n",
       " 'distribution': 'MISSING distribution. '}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomVariable()()\n",
    "RandomVariable()().metadata[rv_dataclass_metadata_key]\n",
    "asdict(RandomVariable()().metadata[rv_dataclass_metadata_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from decorator import decorator\n",
    "from fastcore.basics import patch_to\n",
    "from dataclasses import asdict\n",
    "import pandas as pd\n",
    "from optuna import Trial\n",
    "\n",
    "def is_experiment_setting(cls):\n",
    "    for field in fields(cls):\n",
    "        if not isinstance(field.metadata.get(rv_dataclass_metadata_key, None), RandomVariable):\n",
    "           return False\n",
    "    return True\n",
    "        \n",
    "def show_dataframe_doc(cls):\n",
    "    results = []\n",
    "    for field in fields(cls):\n",
    "        rv = field.metadata.get(rv_dataclass_metadata_key, None)\n",
    "        if rv is None:\n",
    "            raise ValueError(\"Class decorated with @experiment_setting needs to use ~RandomVariable fields. \")\n",
    "        field_info = dict(name=field.name, type=field.type) | asdict(rv)\n",
    "        results.append(field_info)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def get_optuna_search_space(cls, frozen_rvs:set = None):\n",
    "    search_space = {}\n",
    "    for field in fields(cls):\n",
    "        field_name = field.name\n",
    "        if frozen_rvs is not None and field_name in frozen_rvs:\n",
    "            continue\n",
    "        rv = field.metadata.get(rv_dataclass_metadata_key, None)\n",
    "        if rv is None:\n",
    "            raise ValueError(\"Class decorated with @experiment_setting needs to use ~RandomVariable fields. \")\n",
    "        search_space[field_name] = rv.distribution\n",
    "    return search_space\n",
    "\n",
    "from copy import deepcopy\n",
    "def optuna_suggest(cls:Type, trial:Trial, fixed_meta_params, suggest_params_only_in: set = None):\n",
    "    suggested_params = deepcopy(fixed_meta_params)\n",
    "    if suggest_only_in is None:\n",
    "        suggest_only_in = set(field.name for field in fields(cls))\n",
    "    # fixed_meta_params is dataclass\n",
    "    if not isinstance(fixed_meta_params, cls):\n",
    "        raise ValueError(f\"fixed_meta_params should be an instance of the {cls.__name__} class.\")\n",
    "    for field in fields(cls):\n",
    "        if field.name not in suggest_only_in:\n",
    "            continue\n",
    "        rv = field.metadata.get(rv_dataclass_metadata_key, None)\n",
    "        if rv is None:\n",
    "            raise ValueError(\"Class decorated with @experiment_setting needs to use ~RandomVariable fields. \")\n",
    "        suggested_value = trial._suggest(field.name, rv.distribution)\n",
    "        setattr(suggested_params, field.name, suggested_value)\n",
    "    return suggested_params\n",
    "    \n",
    "\n",
    "\n",
    "@decorator\n",
    "def experiment_setting_decorator(dataclass_func, *args, **kwargs):\n",
    "    result_cls = dataclass_func(*args, **kwargs)\n",
    "    if not is_experiment_setting(result_cls):\n",
    "        raise ValueError(\"Class decorated with @experiment_setting needs to use ~RandomVariable fields. \")\n",
    "    patch_to(result_cls, cls_method=True)(show_dataframe_doc)\n",
    "    patch_to(result_cls, cls_method=True)(get_optuna_search_space)\n",
    "    patch_to(result_cls, cls_method=True)(optuna_suggest)\n",
    "    return result_cls\n",
    "\n",
    "experiment_setting = experiment_setting_decorator(dataclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些使用案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.distributions import IntDistribution, FloatDistribution, CategoricalDistribution\n",
    "@experiment_setting\n",
    "class SupportVectorClassifierConfig:\n",
    "    # 惩罚系数 C\n",
    "    C: float = ~RandomVariable(\n",
    "        default=1.0,\n",
    "        description=\"Regularization parameter. The strength of the regularization is inversely proportional to C.\",\n",
    "        distribution=FloatDistribution(1e-5, 1e2, log=True)\n",
    "    )\n",
    "    # 核函数类型\n",
    "    kernel: str = ~RandomVariable(\n",
    "        default=\"rbf\",\n",
    "        description=\"Kernel type to be used in the algorithm.\",\n",
    "        distribution=CategoricalDistribution(choices=[\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"])\n",
    "    )\n",
    "    \n",
    "    # 多项式核函数的度数\n",
    "    degree: int = ~RandomVariable(\n",
    "        default=3,\n",
    "        description=\"Degree of the polynomial kernel function ('poly').\",\n",
    "        distribution=IntDistribution(1, 10, log=False)\n",
    "    )\n",
    "    \n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SupportVectorClassifierConfig(C=1.0, kernel='rbf', degree=3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SupportVectorClassifierConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining `dataclass` (experiment_setting) with PyTorch `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# deprecated\n",
    "# def my_dataclass(cls):\n",
    "#     # https://discuss.pytorch.org/t/typeerror-unhashable-type-for-my-torch-nn-module/109424/5\n",
    "#     cls = dataclass(cls, eq=False) \n",
    "#     old_init = cls.__init__\n",
    "#     def new_init(*args, **kwargs):\n",
    "#         cls.__pre_init__(*args, **kwargs)\n",
    "#         old_init(*args, **kwargs)\n",
    "#     cls.__init__ = new_init\n",
    "#     return cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决 https://discuss.pytorch.org/t/typeerror-unhashable-type-for-my-torch-nn-module/109424/5 中提到的问题，首先定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@decorator\n",
    "def pre_init_decorator(init_func, self, *args, **kwargs):\n",
    "    self.__pre_init__(*args, **kwargs)\n",
    "    return init_func(self, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# TODO decorator style for dataclass_for_torch\n",
    "# @decorator\n",
    "# def dataclass_for_torch_decorator(dataclass_func, cls, eq=False, *args, **kwargs):\n",
    "#     result_cls = dataclass_func(cls, eq=eq, *args, **kwargs)\n",
    "#     result_cls.__init__ = pre_init_decorator(result_cls.__init__, cls)\n",
    "#     return result_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dataclass_for_torch_decorator(dataclass_func):\n",
    "    def wrapped_func(cls):\n",
    "        result_cls = dataclass_func(cls, eq=False) \n",
    "        result_cls.__init__ = pre_init_decorator(result_cls.__init__, self=cls) #TODO 非常奇怪，但是似乎测试逻辑是对的\n",
    "        return cls\n",
    "    return wrapped_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_experiment_module = dataclass_for_torch_decorator(experiment_setting) # 隐藏，不建议直接使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "@_experiment_module\n",
    "class ExperimentModule(nn.Module):\n",
    "    def __pre_init__(self, *args, **kwargs):\n",
    "        # 为什么官方 dataclass 没有 pre init 我气死了。\n",
    "        super().__init__() # torch 的初始化\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        # dataclass生成的init是没有调用super().__init__()的，所以需要手动调用\n",
    "        # https://docs.python.org/3/library/dataclasses.html#dataclasses.__post_init__\n",
    "        # 这里调用PyTorch的init，接下来用户写self.xx = xx就能注册参数、子模块之类的。\n",
    "        # super().__init__() \n",
    "        # 为了防止用户自己忘记写 super().__post_init__() ，我们换个名字方便用户记忆。\n",
    "        self.setup()\n",
    "    def setup(self):\n",
    "        # 用户实现，初始化增量神经网络的增量参数v\n",
    "        raise NotImplementedError(\"Should be implemented by subclass! \")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return super().__repr__()\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return super().extra_repr()\n",
    "    \n",
    "    def __init_subclass__(cls) -> None:\n",
    "        super().__init_subclass__()\n",
    "        original_repr = cls.__repr__\n",
    "        original_extra_repr = cls.extra_repr\n",
    "        # dataclass(cls) # 这个3.10以后是in place的， 不保证？\n",
    "        _experiment_module(cls) # 这个3.10以后是in place的， 不保证？\n",
    "        dataclass_repr = cls.__repr__\n",
    "        def extra_repr(self):\n",
    "            dcr = dataclass_repr(self)\n",
    "            dcr = dcr[dcr.index(\"(\")+1:dcr.rindex(\")\")]\n",
    "            return dcr+original_extra_repr(self)\n",
    "        # cls.extra_repr = lambda self:(dataclass_repr(self)+original_extra_repr(self)) # dataclass的 repr提供给PyTorch\n",
    "        cls.extra_repr = extra_repr # dataclass的 repr提供给PyTorch\n",
    "        cls.__repr__ = original_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDataclassModule(ExperimentModule):\n",
    "    name:str =~RandomVariable(default=\"root\", description=\"Name of the person\")\n",
    "    age:int =~RandomVariable(default=25, description=\"Age of the person\")   \n",
    "    def setup(self):\n",
    "        print(self.name, self.age)\n",
    "        self.linear = nn.Linear(self.age, self.age)\n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor([x])\n",
    "        return self.linear(x)\n",
    "    def extra_repr(self) -> str:\n",
    "        return super().extra_repr()+\" Hello World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 1\n",
      "ExampleDataclassModule(\n",
      "  name='hello', age=1 Hello World!\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "ExampleDataclassModule(\n",
      "  name='hello', age=1 Hello World!\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "tensor([0.4311], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = ExampleDataclassModule(name=\"hello\", age=1)\n",
    "# test = TestFlaxStyle()\n",
    "print(test) # str\n",
    "print(repr(test))\n",
    "print(test(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuequ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
