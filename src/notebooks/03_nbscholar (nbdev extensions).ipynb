{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbscholar Â§ßÁâõÂ≠¶ËÄÖ\n",
    "\n",
    "> nbscholar (new-big scholar, Â§ßÁâõÂ≠¶ËÄÖ) is a extension for better NoteBook development, extending fastai's `nbdev` libary. It is designed to assist you become a New-Big (a.k.a. ÁâõÈÄº or awesome) scholar one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nbscholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "functions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â≠êÊ®°Âùó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from scholarly_infrastructure.logging.nucleus import logger\n",
    "from fastcore.script import call_parse\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def nbscholar_submodules_to_ssh(path: str = \".\"):\n",
    "    \"\"\"\n",
    "    Convert all HTTPS submodule URLs in `.gitmodules` to SSH URLs (e.g. https://github.com/user/repo.git ‚Üí git@github.com:user/repo.git)\n",
    "    \"\"\"\n",
    "\n",
    "    gitmodules = Path(path) / \".gitmodules\"\n",
    "    if not gitmodules.exists():\n",
    "        logger.error(\n",
    "            \"‚ùå Êú™ÊâæÂà∞ .gitmodules Êñá‰ª∂ÔºåËØ∑Âú®Âê´ÊúâÂ≠êÊ®°ÂùóÁöÑÈ°πÁõÆÊ†πÁõÆÂΩïËøêË°åÊ≠§ÂáΩÊï∞„ÄÇ\"\n",
    "        )\n",
    "        raise FileNotFoundError(\".gitmodules not found\")\n",
    "\n",
    "    content = gitmodules.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # ÂåπÈÖç https://github.com/user/repo(.git)\n",
    "    pattern = re.compile(r\"https://([^/]+)/([^/]+)/([^/\\n]+)(\\.git)?\")\n",
    "\n",
    "    changed = False\n",
    "\n",
    "    def https_to_ssh(match):\n",
    "        nonlocal changed\n",
    "        host, user, repo, dotgit = match.groups()\n",
    "        ssh_url = f\"git@{host}:{user}/{repo}.git\"\n",
    "        logger.info(f\"üîÅ ËΩ¨Êç¢: https://{host}/{user}/{repo} ‚Üí {ssh_url}\")\n",
    "        changed = True\n",
    "        return ssh_url\n",
    "\n",
    "    new_content = pattern.sub(https_to_ssh, content)\n",
    "\n",
    "    if changed:\n",
    "        gitmodules.write_text(new_content, encoding=\"utf-8\")\n",
    "        logger.success(\"‚úÖ Â∑≤Êõ¥Êñ∞ .gitmodules Êñá‰ª∂‰∏≠ÁöÑ URL„ÄÇ\")\n",
    "\n",
    "        # ÂêåÊ≠•Â≠êÊ®°ÂùóÈÖçÁΩÆ\n",
    "        res = subprocess.run([\"git\", \"submodule\", \"sync\", \"--recursive\"])\n",
    "        if res.returncode == 0:\n",
    "            logger.success(\"‚úÖ Â∑≤ÊâßË°å `git submodule sync --recursive`„ÄÇ\")\n",
    "        else:\n",
    "            logger.warning(\"‚ö†Ô∏è `git submodule sync` ÊâßË°åÂ§±Ë¥•ÔºåËØ∑ÊâãÂä®Ê£ÄÊü•„ÄÇ\")\n",
    "    else:\n",
    "        logger.info(\"‚ÑπÔ∏è Ê≤°ÊúâÂèëÁé∞ HTTPS Ê†ºÂºèÁöÑÂ≠êÊ®°Âùó URLÔºåÊó†ÈúÄ‰øÆÊîπ„ÄÇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@call_parse\n",
    "def nbscholar_submodules(mode: str = \"to_ssh\", *args, **kwargs):\n",
    "    if mode == \"to_ssh\":\n",
    "        nbscholar_submodules_to_ssh(*args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "\n",
    "functions.append(nbscholar_submodules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## nbscholar_docs\n",
    "\n",
    "alternative to nbdev_docs\n",
    "\n",
    "https://github.com/AnswerDotAI/nbdev/issues/1464\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbscholar_license\n",
    "\n",
    "use google addlicense to add license to all files in the repo, while ignoring gitignore and gitmodules.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbscholar_check\n",
    "\n",
    "Ê£ÄÊü•ipynbÊñá‰ª∂ÊòØÂê¶ÊúâËØ≠Ê≥ïÈîôËØØÔºåÂπ∂‰∏îÂÆö‰ΩçÈîôËØØÁöÑ‰ΩçÁΩÆ„ÄÇ\n",
    "Áî±‰∫é nbdev_export Êä•ÈîôÁöÑÊó∂ÂÄôÊó†Ê≥ïÂÆö‰ΩçÂÖ∑‰ΩìÊòØÂì™‰∏™Á¨îËÆ∞Êú¨Âá∫ÈîôÔºåÊâÄ‰ª•Êàë‰ª¨ÁöÑËøô‰∏™Â∑•ÂÖ∑ÈùûÂ∏∏ÊúâÁî®„ÄÇ\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from scholarly_infrastructure.logging.nucleus import logger\n",
    "\n",
    "def is_export_cell(code: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if the first non-empty line starts with '#| export' or '#| exporti'.\n",
    "    \"\"\"\n",
    "    lines = code.splitlines()\n",
    "    first_idx = next((i for i, l in enumerate(lines) if l.strip()), None)\n",
    "    if first_idx is None:\n",
    "        return False\n",
    "    first = lines[first_idx].strip()\n",
    "    return re.match(r'^#\\s*\\|\\s*exporti?\\b', first) is not None\n",
    "\n",
    "\n",
    "def _normalize_ipython_code(code: str):\n",
    "    \"\"\"\n",
    "    Normalize IPython syntax so we can safely run ast.parse.\n",
    "    Returns:\n",
    "      - str: normalized Python code\n",
    "      - None: skip this cell (non-Python cell magic like %%bash/%%html/%%markdown/etc.)\n",
    "    \"\"\"\n",
    "    lines = code.splitlines()\n",
    "\n",
    "    # Find first non-empty line\n",
    "    first_idx = next((i for i, l in enumerate(lines) if l.strip()), None)\n",
    "    if first_idx is None:\n",
    "        return \"\"\n",
    "\n",
    "    first = lines[first_idx].lstrip()\n",
    "    if first.startswith(\"%%\"):\n",
    "        magic = (first[2:].strip().split()[0].lower() if len(first) > 2 else \"\")\n",
    "        non_python_magics = {\n",
    "            \"bash\", \"sh\", \"html\", \"markdown\", \"js\", \"javascript\",\n",
    "            \"latex\", \"sql\", \"ruby\", \"perl\", \"svg\", \"writefile\", \"cython\"\n",
    "        }\n",
    "        # Non-Python cell magic: skip the whole cell from syntax checking\n",
    "        if magic in non_python_magics or magic == \"\":\n",
    "            return None\n",
    "        # Python-related cell magic: drop the magic line, keep the rest\n",
    "        lines.pop(first_idx)\n",
    "\n",
    "    def is_line_magic(s: str) -> bool:\n",
    "        return s.lstrip().startswith(\"%\")\n",
    "\n",
    "    def is_introspection_line(s: str) -> bool:\n",
    "        t = s.strip()\n",
    "        return t.startswith(\"?\") or t.endswith(\"?\") or t.endswith(\"??\")\n",
    "\n",
    "    filtered = [\n",
    "        ln for ln in lines\n",
    "        if not is_line_magic(ln) and not is_introspection_line(ln)\n",
    "    ]\n",
    "    return \"\\n\".join(filtered)\n",
    "\n",
    "\n",
    "def check_ipynb_file(filepath):\n",
    "    \"\"\"\n",
    "    Check a single ipynb file for Python syntax errors in code cells.\n",
    "    For cells starting with '#| export' or '#| exporti', strictly enforce Python AST (no IPython syntax allowed).\n",
    "    For normal cells, allow IPython syntax (%%, %, ?/??) by normalizing or skipping those lines.\n",
    "    Returns True if no errors, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            notebook = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"‚ùå {filepath} - Invalid JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "    if \"cells\" not in notebook:\n",
    "        logger.warning(f\"‚ö†Ô∏è  {filepath} - No 'cells' key found (malformed notebook)\")\n",
    "        return True  # Not a syntax error in code, skip\n",
    "\n",
    "    errors_found = False\n",
    "\n",
    "    for cell_idx, cell in enumerate(notebook[\"cells\"], start=1):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue  # Skip non-code cells\n",
    "\n",
    "        raw_code = \"\".join(cell.get(\"source\", []))\n",
    "        if not raw_code.strip():\n",
    "            continue  # Skip empty code cells\n",
    "\n",
    "        # Export cells: strict AST on raw code; Non-export: normalized AST\n",
    "        if is_export_cell(raw_code):\n",
    "            normalized_code = raw_code\n",
    "        else:\n",
    "            normalized_code = _normalize_ipython_code(raw_code)\n",
    "\n",
    "        # Skip non-Python IPython cells (e.g., %%bash)\n",
    "        if normalized_code is None or not normalized_code.strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ast.parse(normalized_code)\n",
    "        except SyntaxError as e:\n",
    "            abs_path = str(Path(filepath).resolve())\n",
    "            line = e.lineno or 1\n",
    "            col = e.offset or 0\n",
    "            # VSCode ÂèãÂ•ΩÊ†ºÂºèÔºö/abs/path/to/file:line:column: message\n",
    "            logger.error(f\"{abs_path}:{line}:{col}: SyntaxError: {e.msg} (cell {cell_idx})\")\n",
    "            if e.text:\n",
    "                line_text = e.text.rstrip()\n",
    "                logger.error(line_text)\n",
    "                if col > 0:\n",
    "                    logger.error(\" \" * (col - 1) + \"^\")\n",
    "            errors_found = True\n",
    "\n",
    "    if not errors_found:\n",
    "        logger.info(f\"‚úÖ {filepath} - No syntax errors found\")\n",
    "    return not errors_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from scholarly_infrastructure.logging.nucleus import logger\n",
    "\n",
    "def check_directory(dirpath):\n",
    "    \"\"\"\n",
    "    Check all ipynb files in a directory (and subdirectories) for syntax errors.\n",
    "    Returns True if all files are error-free, False otherwise.\n",
    "    \"\"\"\n",
    "    all_ok = True\n",
    "    file_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    dir_path = Path(dirpath)\n",
    "    # ‰ΩøÁî®rglobÈÄíÂΩíÊü•ÊâæÊâÄÊúâipynbÊñá‰ª∂\n",
    "    for filepath in dir_path.rglob(\"*.ipynb\"):\n",
    "        file_count += 1\n",
    "        if not check_ipynb_file(str(filepath)):\n",
    "            error_count += 1    \n",
    "            all_ok = False\n",
    "\n",
    "    logger.info(\"\\n--- Summary ---\")\n",
    "    logger.info(f\"Checked {file_count} .ipynb files\")\n",
    "    logger.info(f\"Found errors in {error_count} files\")\n",
    "    logger.info(f\"All files syntax error-free: {'Yes' if all_ok else 'No'}\")\n",
    "    return all_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'call_parse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# | export\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;129m@call_parse\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnbscholar_check\u001b[39m(\n\u001b[32m      4\u001b[39m     target_path: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Path to .ipynb file or directory containing .ipynb files\u001b[39;00m\n\u001b[32m      5\u001b[39m ):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    Check .ipynb files for severe Python syntax errors (indent, missing colons/comma, etc.)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(target_path):\n",
      "\u001b[31mNameError\u001b[39m: name 'call_parse' is not defined"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "from fastcore.script import call_parse\n",
    "@call_parse\n",
    "def nbscholar_check(\n",
    "    target_path: str = \".\",  # Path to .ipynb file or directory containing .ipynb files\n",
    "):\n",
    "    \"\"\"\n",
    "    Check .ipynb files for severe Python syntax errors (indent, missing colons/comma, etc.)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Path does not exist: {target_path}\")\n",
    "\n",
    "    if os.path.isfile(target_path):\n",
    "        if not target_path.endswith(\".ipynb\"):\n",
    "            raise ValueError(f\"‚ùå Not a .ipynb file: {target_path}\")\n",
    "        check_ipynb_file(target_path)\n",
    "    else:\n",
    "        check_directory(target_path)\n",
    "\n",
    "\n",
    "functions.append(nbscholar_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbscholar_export\n",
    "\n",
    "alternative to nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import configparser\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# from skinfra.experiment import load_overlaying_config\n",
    "def read_settings_ini(directory:str, item=\"nbs_path\", track=\"DEFAULT\", ini_name='settings.ini'):\n",
    "    # return load_overlaying_config(\n",
    "    #     directory, ini_name\n",
    "    # )[track][item]\n",
    "    config = configparser.ConfigParser()\n",
    "    # ÈÄêÁ∫ßÂêë‰∏äÊü•ÊâæÔºåÁõ¥Âà∞ÊâæÂà∞Á¨¨‰∏Ä‰∏™ÂåÖÂê´ settings.ini ÁöÑÁõÆÂΩï\n",
    "    current = directory\n",
    "    \n",
    "    while True:\n",
    "        settings_path = os.path.join(current, ini_name)\n",
    "        if os.path.exists(settings_path):\n",
    "            break\n",
    "        parent = os.path.dirname(current)\n",
    "        if parent == current:  # Â∑≤Âà∞ËææÊ†πÁõÆÂΩï\n",
    "            raise FileNotFoundError(f\"Could not find {ini_name} in any parent of {directory}\")\n",
    "        current = parent\n",
    "\n",
    "    config.read(settings_path)\n",
    "    assert track in config, f\"Could not find {track} in {settings_path}\"\n",
    "    assert item in config[track], f\"Could not find {item} in {settings_path}\"\n",
    "    return config[track][item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import subprocess\n",
    "import os\n",
    "from scholarly_infrastructure.logging.nucleus import logger\n",
    "@call_parse\n",
    "def nbscholar_export(path:str = \".\"):\n",
    "    res = os.system(\"nbdev_export\")\n",
    "    if res!= 0:\n",
    "        logger.error(\"nbdev_export failed, now I will use `nbscholar_check` to help you locate the error. \")\n",
    "        nbscholar_check(path)\n",
    "        raise Exception(\"nbdev_export failed\")\n",
    "    # ËØªÂèñ settings.ini ÁöÑ lib_path\n",
    "    lib_path = read_settings_ini(path, item=\"lib_path\")\n",
    "\n",
    "    # MKINIT ÁîüÊàê __init__.py\n",
    "    # res = os.system(f\"mkinit {lib_name} -w --lazy_loader --recursive --relative\")\n",
    "    res = os.system(f\"mkinit {lib_path} -w --lazy_loader_typed --recursive --relative\")\n",
    "    if res!= 0:\n",
    "        # raise Exception(\"mkinit failed\")\n",
    "        logger.warning(\"mkinit failed\")\n",
    "    \n",
    "    # RUFF Ê†ºÂºèÂåñ\n",
    "    res = os.system(f\"ruff format {lib_path}\")\n",
    "    if res!= 0:\n",
    "        logger.warning(\"ruff format failed\")\n",
    "functions.append(nbscholar_export)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbscholar_translate\n",
    "\n",
    "https://github.com/AnswerDotAI/nbdev/issues/1429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbscholar_license\n",
    "\n",
    "https://github.com/AnswerDotAI/nbdev/issues/1475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nbscholar_separate\n",
    "\n",
    "https://github.com/AnswerDotAI/nbdev/issues/1468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import nbformat\n",
    "import re\n",
    "from nbformat.notebooknode import NotebookNode, from_dict\n",
    "from scholarly_infrastructure import default_on_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "read_settings_ini_none = default_on_exception(read_settings_ini, default_value=None)\n",
    "\n",
    "\n",
    "def guess_notebooks_path(directory=\".\"):\n",
    "    if isinstance(directory, Path):\n",
    "        directory = directory.as_posix()\n",
    "    # ËØªÂèñ setting.ini ÁöÑ nbs_pathÔºå Â¶ÇÊûúÊúâÁöÑËØùÔºåÊ≤°ÊúâÂ∞±ËøîÂõûNone\n",
    "    return read_settings_ini_none(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly_infrastructure.help import runs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebooks\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ÂàõÂª∫‰∏Ä‰∏™‰∏¥Êó∂ÁöÑ settings.ini Êñá‰ª∂ËøõË°åÊµãËØï\n",
    "with open(runs_path/\"settings.ini\", \"w\") as f:\n",
    "    f.write(\"[DEFAULT]\\nnbs_path=notebooks\")\n",
    "\n",
    "# ÊµãËØï guess_notebooks_path ÂáΩÊï∞\n",
    "print(guess_notebooks_path(runs_path))  # Â∫îËØ•ËæìÂá∫ 'notebooks'\n",
    "\n",
    "# Âà†Èô§‰∏¥Êó∂ÁöÑ settings.ini Êñá‰ª∂\n",
    "os.remove(runs_path/\"settings.ini\")\n",
    "\n",
    "# ÊµãËØï settings.ini Êñá‰ª∂‰∏çÂ≠òÂú®ÁöÑÊÉÖÂÜµ\n",
    "print(guess_notebooks_path(runs_path))  # Â∫îËØ•ËæìÂá∫ None\n",
    "\n",
    "# ÂàõÂª∫‰∏Ä‰∏™‰∏çÂåÖÂê´ nbs_path ÁöÑ settings.ini Êñá‰ª∂ËøõË°åÊµãËØï\n",
    "with open(runs_path/\"settings.ini\", \"w\") as f:\n",
    "    f.write(\"[DEFAULT]\\nother_key=other_value\")\n",
    "\n",
    "# ÊµãËØï guess_notebooks_path ÂáΩÊï∞\n",
    "print(guess_notebooks_path(runs_path))  # Â∫îËØ•ËæìÂá∫ None\n",
    "\n",
    "# Âà†Èô§‰∏¥Êó∂ÁöÑ settings.ini Êñá‰ª∂\n",
    "os.remove(runs_path/\"settings.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_import_and_code_cells(notebook, inplace=True):\n",
    "    \"\"\"\n",
    "    Process a Jupyter Notebook file, splitting cells with both import and non-import lines into two cells.\n",
    "    The first new cell will contain only import statements, and the second will contain the rest of the code.\n",
    "    \"\"\"\n",
    "    notebook = notebook if inplace else deepcopy(notebook)\n",
    "    \n",
    "    new_cells = []\n",
    "\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            # Split the lines in the cell\n",
    "            lines = cell['source'].splitlines()\n",
    "\n",
    "            # Extract leading blank lines or lines starting with \"#|\"\n",
    "            leading_lines = []\n",
    "            while lines and (lines[0].strip() == \"\" or lines[0].startswith(\"#|\")):\n",
    "                leading_lines.append(lines.pop(0))\n",
    "\n",
    "            # Separate import statements and other code lines\n",
    "            import_lines = [line for line in lines if re.match(r\"^\\s*import\\b|^\\s*from\\b\", line)]\n",
    "            other_lines = [line for line in lines if line not in import_lines]\n",
    "\n",
    "            if import_lines and other_lines:\n",
    "                # Add the leading lines to the import cell\n",
    "                \n",
    "                new_cells.append(from_dict(cell | {\n",
    "                    \"cell_type\": \"code\",\n",
    "                    \"metadata\": {},\n",
    "                    \"source\": \"\\n\".join(leading_lines + import_lines), \n",
    "                    \"outputs\": []\n",
    "                }))\n",
    "                # Add the leading lines to the other code cell\n",
    "                new_cells.append(from_dict(cell | {\n",
    "                    \"cell_type\": \"code\",\n",
    "                    \"metadata\": {},\n",
    "                    \"source\": \"\\n\".join(leading_lines + other_lines),\n",
    "                    \"outputs\": cell['outputs']\n",
    "                }))\n",
    "            else:\n",
    "                # If no split is needed, retain the original cell\n",
    "                new_cells.append(cell)\n",
    "        else:\n",
    "            # Retain non-code cells as is\n",
    "            new_cells.append(cell)\n",
    "\n",
    "    # Update the notebook with the modified cells\n",
    "    notebook['cells'] = new_cells\n",
    "    return notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def operate_on_notebook_in(input_path, output_path=None, operation=split_import_and_code_cells):\n",
    "    if output_path is None:\n",
    "        output_path = input_path\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    notebook = operation(notebook)\n",
    "    # Save the modified notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_notebooks_in_folder(folder_path, operation=split_import_and_code_cells):\n",
    "    \"\"\"\n",
    "    Traverse all .ipynb files in a folder and apply the cell-splitting logic.\n",
    "    \"\"\"\n",
    "    if isinstance(folder_path, Path):\n",
    "        folder_path = folder_path.as_posix()\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.ipynb'):\n",
    "                notebook_path = os.path.join(root, file)\n",
    "                print(f\"Processing {notebook_path}\")\n",
    "                operate_on_notebook_in(notebook_path, operation=operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def nbscholar_separate(path:str=\".\"):\n",
    "    if os.path.isfile(path):\n",
    "        return operate_on_notebook_in(path, split_import_and_code_cells)\n",
    "    \n",
    "    notebook_path = guess_notebooks_path(path)\n",
    "    if notebook_path is None:\n",
    "        notebook_path = path\n",
    "    process_notebooks_in_folder(notebook_path)\n",
    "functions.append(nbscholar_separate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "functions_dict = {\n",
    "    v.__name__.replace(\"nbscholar_\", \"\"):v for v in functions\n",
    "}\n",
    "\n",
    "# Êï¥‰ΩìË∑ØÁî±\n",
    "@call_parse\n",
    "def nbscholar(mode:str=\"export\", *args, **kwargs):\n",
    "    function = functions_dict.get(mode, None)\n",
    "    if function is None:\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    function(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "fileId": "31f2d027-914b-4fc6-ab85-8acdb1e874ce",
  "filePath": "/root/repos/peft/BoGuan_YueQu/third_party/infras/ScholarlyInfrastructure/src/notebooks/03_nbscholar (nbdev extensions).ipynb",
  "kernelspec": {
   "display_name": "yuequ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
